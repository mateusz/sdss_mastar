{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the data for export to Sagemaker Autopilot, then reports on the results of Sagemaker auto-ml run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stars\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sl = stars.StarLoader('data/mastarall-v3_1_1-v1_7_7.fits', 'data/mastar-combspec-v3_1_1-v1_7_7-lsfpercent99.5.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodt = sl.stars[sl.stars['INPUT_TEFF']>0]\n",
    "teff = np.array(goodt['INPUT_TEFF']).reshape(len(goodt['INPUT_TEFF']), 1)\n",
    "goodt_array = np.hstack([np.array(goodt['FLUX_CORR']), teff])\n",
    "\n",
    "colcount = goodt_array.shape[1]\n",
    "header = []\n",
    "for c in np.arange(colcount-1):\n",
    "    header.append('flux%d' % c)\n",
    "header.append('teff')\n",
    "\n",
    "df_goodt = pd.DataFrame(goodt_array, columns=header)\n",
    "df_goodt.to_parquet('data/goodt.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I uploaded the resulting parquet file and targeted \"teff\" column with a Sagemaker Autopilot. I set it to generate notebooks only, so that I could step through the optimisation process.\n",
    "\n",
    "Having limited my spend to couple of dollars, I could afford at most 10 hours of runtime. This gave me 100 training jobs at approx 10 hour(s), 35 minute(s) (I had to stop the training early).\n",
    "\n",
    "The hyper-algorithm didn't really seem to improve over time though, as can be seen on the graph below (plots log10(mse) over time):\n",
    "\n",
    "![](./images/sagemaker_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best candidate Sagemaker Autopilot came up with was described such:\n",
    "\n",
    "**[dpp0-xgboost](sagemaker_dpp0.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model.\n",
    "\n",
    "The winner was an xgboost with the following parameters:\n",
    "\n",
    "```\n",
    "alpha\t\t\t1.118370287233794\n",
    "colsample_bytree\t0.7327357874854505\n",
    "eta\t\t\t0.05108342069354128\n",
    "gamma\t\t\t1.4447039174781267\n",
    "lambda\t\t\t0.014694710871502958\n",
    "max_depth\t\t7\n",
    "min_child_weight\t0.2518209934598997\n",
    "num_round\t\t499\n",
    "subsample\t\t0.5854222916407865\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is implemented by AWS in their super-secret docker images, so there is no source.\n",
    "\n",
    "We could try replicating it though - these results are way better compared to what TPOT could come up with for XGBoost (granted, it was crashing a lot). I think perhaps scaling might be important here, so taking a look at the preprocessing steps AWS applied could be good."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e66e1a829a2a82f0cb6aeacf2ea5f44cd8944c8a18096b63b985eee8a9e9eb6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mastar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
